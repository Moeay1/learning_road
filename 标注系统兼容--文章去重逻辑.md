#### 文章去重基本原理

> 根据算法组去重算法计算每篇文章的`fingerprinting`, 将其作为`keyword_id`字段写入. 每个数据`id`都对应一个`keyword_id`, 当两个或多个数据具有同一个`keyword_id`时, 即为重复数据.
>
> 算法组数据清洗脚本中, 会将每个`keyword_id`作为`redis`中的`key`, 存储`set`类型的数据, `set`中存放具有此`keyword_id`的所有数据`id`.
>
> 经与张顺 信哥讨论, `keyword_id`需保证全局唯一.

#### 标注系统兼容去重需更改部分

> 1. 任务分配逻辑需更改
>
>    > 目前任务分配采用数据ID做分配, 每次分配给标注人员50条.
>    >
>    > 采用此文章去重逻辑后, 需更改为按`keyword_id`作为任务的粒度, 根据`keyword_id`获取实际数据`ID`然后根据数据`ID`获取具体数据. 
>
> 2. 标注人员选取重复数据中的某一条
>
>    > 当标注人员点击某个`keyword_id`时, 给其一个此`keyword_id`对应的所有数据`ID`列表, 由标注人员选择重复数据中的某一条进行标注.
>
> 3. 标注完成状态的更改
>
>    > 当标注人员将某个`keyword_id`下重复数据的某一条标注完成后, 所有重复数据中均增加一个字段`marked_type`, 对标注人员的标注的设置为`manual`, 代表人工标注; 其他重复数据设置为`auto`, 并将标注人员更改的信息更新到每条重复数据里.
>
> 4. 导出已标注数据
>
>    > 在工程粒度下, 使用`keyword_id`做`terms`聚合, 获取所有数据, 每个`keyword_id`组中的一条作为历史数据. 以此保证导出数据中无重复数据.
>
> **目前只考虑到以上几点**